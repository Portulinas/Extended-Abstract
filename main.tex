%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

\usepackage[utf8]{inputenc} 


 
% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
VisBig : Visualize Big Data in Real Time
}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Gonçalo Fialho \\ 
        Instituto Superior Técnico \\
        Universidade de Lisboa \\ 
        Av. Rovisco Pais, 1049-001 Lisboa, Portugal \\
        Email: goncalo.f.pires@tecnico.ulisboa.pt }
    %    \and  
    %    Gonçalo Fialho \\ 
    %    Instituto Superior Técnico \\
    %    Universidade de Lisboa \\ 
    %    Av. Rovisco Pais, 1049-001 Lisboa, Portugal \\
    %    Email: goncalo.f.pires@tecnico.ulisboa.pt 
    %    }

        % <-this % stops a space
%\thanks{*This work was not supported by any organization}% <-this % stops a space
%\thanks{$^{1}$H. Kwakernaak is with Faculty of Electrical Engineering, Mathematics and Computer Science,
%        Instituto Superior Técnico, Universidade de Lisboa, Portugal
%        {\tt\small h.kwakernaak at papercept.net}}%
%\thanks{$^{2}$P. Misra is with the Department of Electrical Engineering, Wright State University,
%        Dayton, OH 45435, USA
%        {\tt\small p.misra at ieee.org}}%
%}
\usepackage{float}

\makeatletter
\let\NAT@parse\undefined
\makeatother

\usepackage[pdftex]{hyperref}

\usepackage{graphicx}

\begin{document}



\maketitle
\thispagestyle{plain}
\pagestyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Nowadays information devices can be found anywhere, whether in a personal computer, smartwatch or even in our appliances. These devices create relevant information to the identification of patterns or anomalies of the systems in question. Given the significant growth of those devices, some display methods are no longer able to respond effectively to users needs. Although there are solutions to visualize this type of information it's important to look for new ways to present them in a real-time environment so users can act quickly to the abrupt change of the regular flow of data so they can attempt to soften or solve the problems of the systems under analysis. This thesis provides a solution for presenting large amounts of real-time data where users are able to identify anomalies and trends in information as well as maintain context when changes in data flow exist.

We propose \textit{VisMillion}, a visualization interface for large amounts of data in real time. Based on \textit{CANVAS} technology that allows graphical rendering through \textit{Javascript}. This web-based approach enables any device that supports HTML5 run the interface without the need of installation of additional libraries, or the existence of OS dependencies. Using different modules the system represents data in many ways. The more recent the data, the more detailed it is, so each module uses different techniques to aggregate and process information. 

This report does a state-of-the-art analysis for viewing large amounts of data in real time. Then it presents the architecture of the solution (backend / real-time and interface / visualization) and its justifications for the options taken. Finally carries out a detailed evaluation of the obtained results and a final analysis and reflection where some of the future work is also mentioned which could improve the system.

Keywords -  Large ammount of data, BigData, Real-Time, Streaming, Visualization, Flow, Trend, Outliers, Patterns

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}


With exponential increase of devices capable of generating information (from the simplest smarthphones to supercomputers) it was necessary to find ways to make the visualization simple and effective, so the user won't be misled by elements that are not part of the data. Many systems create a lot of data that must be analyzed, companies like \textit{Amazon} or \textit{Facebook} need to keep track of all the activities performed by the users so they can interpret trends and patters to create marketing strategies for their business. Robots and Complex Networks generate logs that are important to analyze in case of failures. This is some examples that lead to the emergence of big chunks of information and it is therefore increasingly common to observe the growth of the storage capacity of the devices.

This agglomerated information that creates large chunks of data is typically called as \textbf{Big Data}. However, the term does not have a specific definition and there is no exact value to a dataset can be considered as Big Data \cite{WardB13a}. This data is so large that the tools that process this information cannot do it in a tolerable time interval and its necessary to use more efficient methods. One of the most common uses for this kind of data is to visualize it through representations that show patterns and particular details about them. Its representation can be done from a greater degree of detail of each point to aggregations that describe a lot of points with just an element. Yet, the large amounts of data makes the discovery of relevant items more difficult causing a more complex and time-consuming analysis. Furthermore its hard to represent all the information just in once because the resolution of the displays could not be enough to represent all the points and Humans have difficulties to understand small parts of data when the datasets are huge, losing the context of their search. To overcome this, data is represented with different levels of detail aggregating information at some point. 

The rise of distributed systems raises this problem to a new scale where it is necessary to monitor real-time changes in case of need for rapid intervention in problems that can occur. The more the servers are the more data will exist and each one have different workload making the amount of data generated by second different and that could cause abrupt changes in the visualization of these systems.

There are challenges that visualization of large amounts of data in real time can arise. First, the processing and rendering phases of this data can be slow. Second, unlike a static environment where the data is always on the same spot, real-time data is constantly coming and there is no way to keep the representation state unmodified and explorable like in a fixed dataset. Third, there is no time to wait for new data to process statistics, this process should always be done the fast as possible so the user could retrieve and get the information to act if necessary. Additionally is important to understand the data domain that is visualized so it can be possible to prevent failures and the systems could adapt to data changes so the users cannot be harmed. 

With this in mind we have created VisMillion, which is a interface that represents large amounts of data in real time. Our main goal is to \textbf{provide new information visualization techniques capable of representing large amounts of data in real time that allows the user to perceive the global context of the information as new packages are received}.

The user should be capable to get a global overview of the system and detect interesting patterns using sub visualizations he will explore more details about it. To make this happen the information will be aggregated over time so recent data is always more detailed than the old one, using statistical methods the data is represented in many ways. User should also be capable to identify \textit{outliers} (anomalies) using different methods that are described in the next sections.

The system composed by 3 different modules positioned side by side, each one represents data with a different level of aggregation. Then data flows through each one from right to left. This is built to be suitable with the debit differences and keep updated to all changes that can occur without misleading the user. 

The application was tested using usability user tests with 21 users, using the same approach to all. The results show that the application suits its purpose and the users are capable of analyze changes, patterns and anomalies.

The rest of the paper is organized as follows. Section \ref{section:stateofart} we discuss the related work about real time visualization of large amounts of data. In section \ref{section:streammer} is described the global system architecture and server implementation. In section \ref{section:background} we give a overview about the methods and requirements about information visualization. In section \ref{section:frontend} we present the interface VisMillion. In \ref{section:evaluation} the evaluation of the system is described. Finally, in section \ref{section:conclusions} the conclusions of the work are presented as the future work.




\section{State of the art}
\label{section:stateofart}
Applying real-time to data visualization creates new types of challenges \cite{7994551}. First, we need to understand the data domain, this process is called \textit{Orientation} and we should analyze the data behavior and its components. Second, the systems should react efficiently to data changes that are unexpected, this phase is called \textit{Re-Orientation}.

Information Visualization has three characteristics that should be thought of: \textit{Technique}, \textit{Interactivity}, \textit{Data}. The first one is about \cite{981847} the representation of the data (2D, 3D, pixels, etc.). The interactivity refers how the user should interact with the data to understand the whole system. Data is about its dimensionality and characteristics that influence how data should be presented.

Khan et al. \cite{1878880183} propose a \textit{framework} interact with real-time data capable of dealing with intense data flows. This interface is capable to detect anomalies and highlight them. This approach perform dimensional reduction to the data and then using a recommendation system it chooses the suitable representations for the data. The information uses smooth transitions, so the user can maintain the context about the global data view.

Systems like \cite{traub2017i2, 7338157} use dimensional reduction to deal with large datasets, therefore the interfaces do not have lots of data to render. These processes are commonly performed at server side due to connection issues like latency and protocols. Furthermore, there is some complexities associated with performing dimensional reduction and usually they take some time to process, there are some algorithms to perform this quickly on real-time systems.

Some monitoring systems like LiveRac \cite{McLachlan} and X-SimViz \cite{7338157} create solutions that enables users to visualize and analyze clusters of nodes that need to be supervised in case of something bad happen and Human intervention is required. These applications use multiple views, each one uses different levels of detail or present different types of data. Using techniques like \textit{Brushing \& Linking} and \textit{Stretch and Squish Navigation} each part of the interface can communicate with the others and make the context of the data received being preserved.

\textit{Event Visualizer} \cite{Fischer} is a system capable of processing, analyzing and presenting dynamic events in real-time. It uses a module that pre-processes events and makes a search for missing data. Then the interface categorizes each event and each category corresponds to a bar called \textit{relaxed timeline}. There are multiple \textit{relatex timelines} and each one is binded with a different color, its width corresponds to a time interval. The user uses \textit{Pan \& Zoom} to interact with the system and watch different timelines avoiding big chunks of events.

After all, these systems try to use multiple techniques to deal with real-time and large datasets. Using different interaction techniques the user is able to perform a better exploration, always keeping in mind that the overview of the system should be maintained and using smooth transitions between the views. Data reduction is used to relieve interface workload and make the elements rendering faster. Some of them even use statistical history about past visualization so they can deduce witch idioms are better for some cases. Data analytic users should be capable to understand patterns and anomalies without much effort, so the interfaces need to be simple and consistent while the data is being received and abrupt system changes should never happen. Despite some systems use these techniques to create interfaces, there is some lack of understanding of how to use all this in one because there are a lot of dependencies with each case.



\section{Streamer}
\label{section:streammer}

The system architecture for this project is based on a client-server approach where the client is the interface, and the server (also called streamer) sends the data to the interface. Since the main purpose of this work is related with the representation of the data, it was decided that the system would follow the \textit{fat-client} architecture where the interface is responsible for the data processing. This web implementation allows the system to be run on every machine that has browser support, so there are no OS dependencies and any regular user is capable use the interface.

Using the \textbf{HTML5} it was possible to make a bidirectional connection between the interface and server so they can communicate to each other, the tool used to make this happen is called \textit{WebSockets}. Data is sent by the server, and the interface receives and processes it, for testing purposes the user is capable to communicate with the server to change the data flow, further ahead this implementation can be used to process data from the server with requisites settled by the user.

Figure \ref{fig:server_struct} shows the application architecture, the dataset is retrieved by a python script (\textit{Streamer.py}) that has which later sends each row of the dataset in one packet to the interface via \textit{WebSocket}, the user must identify the address of the streamer, when this connection is established the streamer starts to send the packages. Again, for testing purposes, the data flow can be exchanged by the user, although in a real environment data is received out of order and a flow cannot be defined.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/server_Structure.png}
    \caption{System architecture}
        \label{fig:server_struct}
\end{figure}

\subsection{Dataset}
\label{subsection:dataset}
Since the purpose of this work is to use a large amount of data to be sent visualized in real-time the dataset to choose must follow two criteria: \textbf{Large} and \textbf{Time-based}. The most common datasets for this are event-based. This data has a timestamp associated (time-based) and one or more attributes, the events can be represented as packages that are sent to the client and then represented. The search of this type of data is facilitated by exponential growth of technology \cite{6567202}, online shopping, social networks and others are always creating this kind of information that can be analyzed and fits perfectly in this case.

It was used two fonts to retrieve this kind of datasets: \textbf{Kaggle} and \textbf{Google BigQuery}. Each one contains multiple time-based data sources with large amounts of data. Using Google BigQuery, it was obtained data related Bitcoin transactions that suit entirely for the system purpose, with this dataset we are able to analyze trends and when there were more transactions occurring or not, as some anomalies that are not in the regular patterns.

The final dataset in CSV format contains hundreds of thousands rows where which one corresponds to transaction value with a timestamp associated. Note this dataset contains only information about half a day of the whole dataset which can be found in Google BigQuery.

\subsection{Usability Tests Server Update}
\label{subsection:usabilityTests}
Due to the need for usability tests to evaluate the system, the server implementation was changed to support data timestamps, so the server could send the data like in a real scenario with different delays between the data dispatch. The retrieved data from the CSV file has now one required field beside the package value, the delta milliseconds between the next package. The server uses this value to set the thread sleep interval between each message sent to the client. This implementation makes more sense than the first one because in the previous approach the interval between each message was always the same, this its very unlikely to happen in a real world context.

\subsection{Dataset Generator}
\label{subsection:datasetGenerator}
The dataset generator is a script created due the need to replicate the exact same views in usability test phase. This script generate CSV files with a pair of value and \textit{delta}. The dataset is processed by the server and then the packages are sent with the interval specified by the value of \textit{delta}. The generator reads the functions called by the user with some parameters and will output a dataset. This functions are:
\begin{itemize}
    \item \textbf{Linear\_Generation} - Generate data points following a linear regression between a given time interval and with a value range (from - to).
    \item \textbf{Constant\_Generation} - Generate data points given a time interval and with a value range.
    \item \textbf{Mark\_Outlier} - Append a outlier to the dataset.
\end{itemize}
For Linear\_Generation and Constant\_Generation the user must provide the size of that generation, this will create the number of packages given by this parameter.\newline 
The values generated by this functions are always randomly generated, using different inputs we can specify how sparse the data points can be and the chance of generate data outside the given value range (Alfa Random) and its limit (Prob Int).

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/exemplo_datageneration.png}
    \caption{Generated \textit{dataset} - Example}
        \label{fig:example_datageneration}
\end{figure}

Figure \ref{fig:example_datageneration} demonstrates a generated dataset using Constant\_Generation on the first phase, followed by a Linear\_Generation and then a Constant\_Generation.

\section{Background - InfoVis}
\label{section:background}

\section{Front End - Visualization}
\label{section:frontend}
This application use D3 framework \cite{2011-d3} to help the creation of the dynamic visualizations. The system only supports one dimensional variables (without including its timestamp). 

The first thoughts about the interface began with the idea that the data should have a graceful degradation over time. Recent data should be way more detailed than old data because there are some characteristics that the users must be aware of during its analysis. As the information navigates through the interface packages are aggregated over some statistical methods such medians, averages, quartiles, frequencies, and others, so the user can be focused on the latest information without losing the context of information that has already been received.

Using this approach emerged a system based on visualization modules, each one presents the information in a different idiom allowing the client to have a more comprehensive experience of the information they are analyzing. The modules arranged side by side aim to create an innovative experiment in the investigation of datasets.

The connection between modules allows that information flows through them from right to left. The side by side approach creates a continuous vision about the system and connects all the information received, helping the analysis of the user in all data flow using different metrics in different moments of the chart. This method allows other users being able to develop another modules for they monitoring systems accordingly their sights and data domain.

It were implemented 3 modules: \textit{Linechart}, \textit{Barchart} and \textit{Scatterchart}. Each one represents one visualization idiom. The goal was to found navigation methods that allow the user to understand the data flow in real time, each one presents different metrics, scattercharts draws points that represents each package, linecharts represents the data tendencies and barcharts (or histograms) the information aggregation between values.
Each time the packages pass through a new module they are aggregated in some way and they should be transformed using smooth animations to their new form making that "graceful degradation" and letting the user won't be misled by abrupt disappearances or changes in the elements. 

For the interaction with each module it was decided that the interface should be as simple as possible and not lead the user to a complex exploration of the information. The user is able to mouse hover the interface, this triggers a tooltip of the corresponding module and displays the information of the overlapping elements. Since this information is always moving (real-time), it was decided that this information should not be updated as the elements navigate through the chart, otherwise the user would have more difficulty tracking the mouse by the elements. This means that tooltip information is only updated when there is mouse movement created by the user.

In one hand, the visualization of the global chart allows an analysis that corresponds to free exploration of the information. On the other hand, the module visualization allows the user to make another search tasks depending on the type of data that the user wants to extract from a specific idiom. Searches can be done by time interval or directly orientated of the median values, using different methods there are some characteristics that can be found as anomalies or data aggregations. \newline 
These methods allow requirements like \textit{Expressiveness} and \textit{Effectiveness} \cite{Mackinlay} to be successfully followed since the main purpose of the system is that user won't be lost in its data visualization navigation and can have explicit information about the data visualized by modules.

\subsection{Implementation}
\label{subsection:implementation}
In the first implementation approach it was used SVG elements to create the charts. Due the lack of scalability (when it comes about large amounts of data) there was a need to rethink how the web interface should behave and how was goint to be implemented in order to allow large amounts of data. The final solution was reached using \textit{canvas} API that allows users draw elements in a a rectangle using \textit{Javascript} to make the processing along with D3 framework. Dispite this solution has better performance results than manipulating SVG elements, D3 does not support element animations on \textit{canvas}. 

The class architecture for the system is presented at \ref{fig:uml}, the main class is called \textbf{Chart} and its responsible to coordinate all the modules that belong to it and with the server, data is stored at this module and afterwards filtered to each component. Since the interface follows the \textit{Update \& Draw} architecture there are 3 fundamental methods on these classes, \textbf{Chart} makes those calls for each module that exists. There is a abstract class called \textbf{Module} and all other three modules inherit from it (linechart, scatterchart, linechart). The first method is \textit{Update} and represents the processing phase of the data for each module. Second method is \textit{Draw}, at this stage all the elements from that component are printed to \textit{canvas}. The last method is the \textit{mouseHover} event that triggers a tooltip and presents the information about the hovered element. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/uml.png}
    \caption{UML classes from \textit{VisMillion}}
        \label{fig:uml}
\end{figure}

The \textbf{Connection} component is a extension from Chart that creates the communication between the server and the interface, this class receives the packages and then appends it with a timestamp (if the packages does not came with one) to the data array saved by the \textbf{Chart}. This extension also allows the user to change the data flow in testing phase.



\subsubsection{Chart}
\label{subsubsection:chart}

\begin{figure*}[ht]
    \centering
  \includegraphics[width=\linewidth]{Figures/chart.png}
  \caption{Chart Overview}
  \label{fig:chart}
\end{figure*}

This system allows the interaction through all the modules so the \textit{Interactive Exploration} requirement can be ruled by this method. The data navigation implemented by the system let the locality of changes and preservation of the temporal context requisites being fulfilled by the interface.\newline
The chart is responsible to maintain this techniques align to each component, using the same scale function and domains to all the modules environments. These attributes are assigned by the users on the creation of the \textit{Chart} and usually cannot be updated in run time. Figure \ref{fig:chart} represents a Chart and its modules.

\subsubsection{Modules}
\label{subsubsection:modules}
The initialization of each module have some global attributes that need to be set like \textit{deltaRange} that describes the time interval in seconds the module should present. The width is evenly distributed by all the modules of a Chart. In the update phase each module asks the Chart for the data packages that are in its data interval. Since the packages navigate through each module from right to left the change between modules must be slowly degraded so the user won't lose its visualization context.

The implemented modules were carefully chosen given its visualization complexity (how familiar are the users to them and if the idiom is easy to understand). Globally, they complement each other well, and the required tasks can be executed with them: Flow and Trend analysis, identification of anomalies and non-typical aggregations, medians, max and min, etc.


\subsubsection{Barchart}
\label{subsubsection:barchart}
\textbf{Barchart} module does not need a time interval since its horizontal axis does not represent time like the other modules. Instead, it represents a frequency. For this reason, this module should be located the most left as possible. Each bar represents the amount of packages received in a given data value interval, this will create a phenomenon of aggregation leads the user to understand the intervals where there are more packages with a certain value. \newline 
This approach takes into account the maintenance of the temporal context of all the information already receibed so far and making the information analysis more complete in a long term. Figure \ref{fig:barchart} shows a Barchart module.

As it is added new data to the bars, the bar that received a package blinks and grows slowly with a transition so the user can understand the moments where new information is arriving to the module.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/barchart.png}
    \caption{Barchart}
        \label{fig:barchart}
\end{figure}

Using the \textit{mouseHover} function the user is able to get the interval of the values of the bar and the frequency of packages placed in that interval.

\subsubsection{Linechart}
\label{subsubsection:linechart}
With the \textbf{Linechart} module the user is capable to understand patterns and tendencies about the information. The horizontal axis now represent the time interval set by the user. It was created a line connection between all the points that belong to the time interval. However in high amounts of data this approach wouldn't work because the user won't be capable to understand the condensed data points. So it were created two states, the \textit{high} and \textit{low} flow. Low flow connects all the packages, while High flow aggregate the data using \textit{Boxplots} approach, the module is divided by sections and then quartiles, median, max and min are calculated for each section. After this calculations a linechart connects all the sections with a different color for each metric creting a more understandable visualization. Figure \ref{fig:linechart} represents a linechart, where top and bottom lines are the maximum and minimum values respectively. Median is given by the orange line and 1º quartile and 3º quartile by the red and purpule colors.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/linechart.png}
    \caption{Linechart}
        \label{fig:linechart}
\end{figure}

The interactions with this module shows the values for each attribute of a section.

\subsubsection{Scatterchart}
\label{subsubsection:scatterchart}
This visualization idiom can represent the data flow very easily, letting the user know what are the moments where exist more data incoming. This is the module that presents the most detailed information since all received packets are rendered. 

Like Linechart, Scatterchart also has the \textit{high} and \textit{low} flow states. The reason why this is needed is because there is a limit where FPS drop substantially, making the visualization not understandable anymore. The low flow state represents the all the packages received as a dot. The high flow state creates a heatmap in which the areas where the more colored the regions are the higher the concentrations of points will be. Unlike Linechart, the flow states in this module can coexist, overlapping each other and creating a simpler analysis of patterns.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/scatterchart.png}
    \caption{Scatterchart}
        \label{fig:scatterchart}
\end{figure}

The interaction performed with this module results in the overlap of the mouse in the squares indicating the time interval and values associated with that packet as well as the number of elements contained in it.

\subsubsection{Outliers}
\label{subsubsection:outliers}
This module was created to represent the points that the user considers to be outliers. As in other works \cite{4015444}, this type of data is treated in separate views for a better understanding and coherence of the system. For this work the outliers are all values that are above the vertical domain defined by the user.

Being a component that does not require so much attention of the user (since the presence of these values must be atypical), the complexity of representation must be smaller than the other modules. This graphic is positioned at the top of the interface since the values represented in it are always larger than the domain (context maintenance).
The module occupies the entire width of the Chart allowing a more lasting visualization of the atypical values occurred.

% https://www.scribens.com/
% https://www.thesaurus.com/
\section{Evaluation}
\label{section:evaluation}

\section{Conclusions}
\label{section:conclusions}



\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{unsrt}
\bibliography{abstract.bib}

\end{document}
